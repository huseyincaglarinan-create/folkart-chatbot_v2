import os
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_pinecone import PineconeVectorStore
from langchain.memory import ConversationBufferWindowMemory

from pinecone import Pinecone
from pinecone_text.sparse import BM25Encoder

# 1. Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_index_name = os.getenv("PINECONE_INDEX_NAME")

# 2. Pinecone baÄŸlantÄ±sÄ±
pc = Pinecone(api_key=pinecone_api_key)
index = pc.Index(pinecone_index_name)

# 3. Embedding modeli
embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)

# 4. VektÃ¶r deposu
vectorstore = PineconeVectorStore(
    index_name=pinecone_index_name,
    embedding=embedding_model,
    text_key="text"
)

# 5. BM25 encoder (dummy init)
bm25 = BM25Encoder()
bm25.fit(["dummy"])

# 6. HafÄ±za (kullanÄ±cÄ± bazlÄ±)
user_memories = {}

# 7. Proje takibi (kullanÄ±cÄ± bazlÄ±)
user_projects = {}

# 8. SatÄ±ÅŸ ofisi konumlarÄ±
PROJECT_LOCATIONS = {
    "Mona": {
        "address": "Folkart Towers, Manas BulvarÄ±, BayraklÄ± / Ä°zmir",
        "maps_url": "https://www.google.com/maps?q=Folkart+Towers,+Manas+Bulvar%C4%B1,+Bayrakl%C4%B1,+%C4%B0zmir"
    },
    "Nova": {
        "address": "Folkart Vega, Mersinli Mah. 2819/3 Sk. No:1 / Ä°zmir",
        "maps_url": "https://www.google.com/maps?q=Kaz%C4%B1mdirik,+296.+Sk.+No:18,+Bornova,+Ä°zmir"
    },
    "Terra": {
        "address": "Åifne Mah. Reisdere Mevkii No: 7 Ã‡eÅŸme/Ä°zmir",
        "maps_url": "https://www.google.com/maps?q=Folkart+Terra+Projesi,+Åifne,+Ã‡eÅŸme,+Ä°zmir"
    }
}

# 9. Prompt ÅŸablonu
template = """
Rol: SatÄ±ÅŸ DanÄ±ÅŸmanÄ±  
Ton: Kibar, gÃ¼ven veren, mÃ¼ÅŸteri odaklÄ±, yÃ¶nlendirici, samimi  

Sen Folkart YapÄ±â€™da Ã§alÄ±ÅŸan deneyimli bir satÄ±ÅŸ danÄ±ÅŸmanÄ±sÄ±n.  
MÃ¼ÅŸterilere karÅŸÄ±:  
- Profesyonel, gÃ¼ven verici ve samimi bir dil kullan.  
- CevaplarÄ±n hem bilgilendirici hem yÃ¶nlendirici olsun.  
- MÃ¼ÅŸteriye deÄŸer kat, ihtiyaÃ§larÄ±nÄ± anlamaya Ã§alÄ±ÅŸ ve sohbeti canlÄ± tut.  
- Belgelerde yer alan bilgilerin dÄ±ÅŸÄ±na Ã§Ä±kma, ama bilgiyi sohbet havasÄ±nda aktar.  

Kurallar: 
    - EÄŸer kullanÄ±cÄ± proje belirtmemiÅŸse, Mona, Nova ve Terra projeleri hakkÄ±nda genel bilgi ver.
    - Her CÃ¼mlenin BaÅŸÄ±na Merhaba Ekleme.
    - soruya net ve kÄ±sa cevap ver.  
    - mÃ¼ÅŸteriye bir sonraki adÄ±mÄ± Ã¶ner.  
    - mÃ¼ÅŸteriyi ihtiyaÃ§larÄ±nÄ± aÃ§maya davet et.  

2. EÄŸer belgede net bilgi yoksa:  
   - â€œbu konuya dair Bilgim BulunmamaktadÄ±r.â€ de,  
   - ama ardÄ±ndan mÃ¼ÅŸteriye yardÄ±mcÄ± olacak **benzer bilgi veya alternatif Ã¶neri** sun.  

3. Her cevabÄ± 5â€“7 cÃ¼mleyle sÄ±nÄ±rla.  

4. Her cevabÄ±n sonunda bir **aksiyon Ã§aÄŸrÄ±sÄ±** yap:  
   - SatÄ±ÅŸ ofisine davet  
   - Daha fazla bilgi paylaÅŸma veya ekiple iletiÅŸim saÄŸlama  
   - Randevu oluÅŸturamÄ±yorsun; iletiÅŸim bilgilerine yÃ¶nlendir.  

Kontekst:
{context}

Soru:
{question}
"""
prompt = ChatPromptTemplate.from_template(template)

# 10. Proje tespiti
def detect_project(query: str, user_id: str) -> str:
    q = query.lower()
    if "mona" in q:
        project = "Mona"
    elif "nova" in q:
        project = "Nova"
    elif "terra" in q:
        project = "Terra"
    else:
        project = user_projects.get(user_id, "Mona")  # Mona varsayÄ±lan

    user_projects[user_id] = project
    return project

# 11. Hybrid Retriever
def hybrid_retrieve(query: str, user_id: str, top_k: int = 12):
    project = detect_project(query, user_id)

    dense_q = embedding_model.embed_query(query)
    sparse_q = bm25.encode_queries([query])[0]

    index.query(
        vector=dense_q,
        sparse_vector=sparse_q,
        top_k=top_k,
        include_metadata=True,
        filter={"project": project}
    )

    docs = vectorstore.similarity_search(query, k=top_k, filter={"project": project})
    context = "\n".join([doc.page_content for doc in docs])
    return context

# 12. Konum isteÄŸini algÄ±layan fonksiyon
def is_location_request(query: str) -> bool:
    q = query.lower()
    return any(word in q for word in ["konum", "adres", "satÄ±ÅŸ ofisi", "lokasyon", "nerede", "nasÄ±l gelirim"])

# âœ… 13. LLM
llm = ChatOpenAI(
    openai_api_key=openai_api_key,
    model="gpt-4.1-mini",
    temperature=0.4,
    max_tokens=400
)

# 14. Ana cevap fonksiyonu
def get_rag_answer(query, user_id="local_test"):
    if user_id not in user_memories:
        user_memories[user_id] = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True,
            k=5
        )

    if is_location_request(query):
        project = detect_project(query, user_id)
        loc = PROJECT_LOCATIONS.get(project)
        if loc:
            address = loc["address"]
            maps_url = loc["maps_url"]
            html_link = f'<a href="{maps_url}" target="_blank"><b>{project} SatÄ±ÅŸ Ofisi Konumu</b></a>'
            return (
                f"ğŸ“ <b>{project}</b> projesinin satÄ±ÅŸ ofisi:<br>"
                f"{address}<br>"
                f"ğŸ—ºï¸ Harita: {html_link}"
            )
        else:
            return "âŒ Åu an bu proje iÃ§in konum bilgisine ulaÅŸamÄ±yorum."

    context = hybrid_retrieve(query, user_id)
    formatted_prompt = prompt.format(context=context, question=query)
    result = llm.invoke(formatted_prompt).content
    user_memories[user_id].save_context({"question": query}, {"answer": result})
    return result




