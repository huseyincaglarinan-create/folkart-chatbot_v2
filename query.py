import os
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableMap, RunnablePassthrough
from langchain_pinecone import PineconeVectorStore
from langchain.memory import ConversationBufferMemory

from pinecone import Pinecone
from pinecone_text.sparse import BM25Encoder

# load environment variables
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_index_name = os.getenv("PINECONE_INDEX_NAME")

# Pinecone Index
pc = Pinecone(api_key=pinecone_api_key)
index = pc.Index(pinecone_index_name)

# Embeddings
embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)

# VectorStore
vectorstore = PineconeVectorStore(
    index_name=pinecone_index_name,
    embedding=embedding_model,
    text_key="text"
)

# BM25 encoder (Ã¶nceden fit edilip kaydedilmeli; burada dummy kullanÄ±m var)
bm25 = BM25Encoder()
bm25.fit(["dummy"])  # RAM taÅŸmasÄ±nÄ± Ã¶nlemek iÃ§in sadece boÅŸ init

# HafÄ±za
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Prompt Template
template = """
Rol: SatÄ±ÅŸ DanÄ±ÅŸmanÄ±  
Ton: Kibar, gÃ¼ven veren, mÃ¼ÅŸteri odaklÄ±, yÃ¶nlendirici, samimi  

Sen Folkart YapÄ±â€™da Ã§alÄ±ÅŸan deneyimli bir satÄ±ÅŸ danÄ±ÅŸmanÄ±sÄ±n.  
MÃ¼ÅŸterilere karÅŸÄ±:  
- Profesyonel, gÃ¼ven verici ve samimi bir dil kullan.  
- CevaplarÄ±n hem bilgilendirici hem yÃ¶nlendirici olsun.  
- MÃ¼ÅŸteriye deÄŸer kat, ihtiyaÃ§larÄ±nÄ± anlamaya Ã§alÄ±ÅŸ ve sohbeti canlÄ± tut.  
- Belgelerde yer alan bilgilerin dÄ±ÅŸÄ±na Ã§Ä±kma, ama bilgiyi sohbet havasÄ±nda aktar.  

Kurallar:  
   - soruya net ve kÄ±sa cevap ver.  
   - mÃ¼ÅŸteriye bir sonraki adÄ±mÄ± Ã¶ner.  
   - mÃ¼ÅŸteriyi ihtiyaÃ§larÄ±nÄ± aÃ§maya davet et.  

2. EÄŸer belgede net bilgi yoksa:  
   - â€œ bu konuya dair Bilgim BulunmamaktadÄ±r.â€ de,  
   - ama ardÄ±ndan mÃ¼ÅŸteriye yardÄ±mcÄ± olacak **benzer bilgi veya alternatif Ã¶neri** sun.  

3. Her cevabÄ± 5â€“7 cÃ¼mleyle sÄ±nÄ±rla (uzun ve sÄ±kÄ±cÄ± olmasÄ±n).  

4. Her cevabÄ±n sonunda bir **aksiyon Ã§aÄŸrÄ±sÄ±** yap:  
   - SatÄ±ÅŸ ofisine davet  
   - Daha fazla bilgi paylaÅŸma veya ekiple iletiÅŸim saÄŸlama  
   - Bu aÅŸamada sen randevu oluÅŸturamÄ±yorsun, mÃ¼ÅŸterileri iletiÅŸim bilgilerine yÃ¶nlendir

Kontekst:
{context}

Soru:
{question}
"""
prompt = ChatPromptTemplate.from_template(template)

# ðŸ”Ž Proje adÄ±nÄ± tespit et
last_project = "Mona"  # VarsayÄ±lan proje

def detect_project(query: str) -> str:
    """
    KullanÄ±cÄ±nÄ±n sorusunda proje adÄ± varsa onu seÃ§er,
    yoksa en son kullanÄ±lan projeyi dÃ¶ner.
    """
    global last_project
    q = query.lower()
    if "mona" in q:
        last_project = "Mona"
    elif "nova" in q:
        last_project = "Nova"
    elif "terra" in q:
        last_project = "Terra"
    return last_project

# Sorguyu geniÅŸlet (query expansion)
def expand_query(query: str):
    llm = ChatOpenAI(openai_api_key=openai_api_key, model="gpt-4.1-mini", temperature=0)
    prompt = f"Soruya 3 eÅŸ anlamlÄ± veya benzer varyant Ã¼ret: {query}"
    expansions = llm.invoke(prompt).content.split("\n")
    return [query] + [e.strip("- ") for e in expansions if e.strip()]

# Hybrid retriever (dense + sparse) + filtre
def hybrid_retrieve(query: str, top_k: int = 12):
    project = detect_project(query)  # âœ… ilgili proje
    all_queries = expand_query(query)
    matches = []
    for q in all_queries:
        dense_q = embedding_model.embed_query(q)
        sparse_q = bm25.encode_queries([q])[0]
        res = index.query(
            vector=dense_q,
            sparse_vector=sparse_q,
            top_k=top_k,
            include_metadata=True,
            filter={"project": project}  # âœ… sadece ilgili proje
        )
        matches.extend(res.matches)

    # Vectorstore Ã¼zerinden context'i getir (aynÄ± filtreyle)
    docs = vectorstore.similarity_search(query, k=top_k, filter={"project": project})
    context = "\n".join([d.page_content for d in docs])
    return context

# LLM
llm = ChatOpenAI(
    openai_api_key=openai_api_key,
    model="gpt-4.1-mini",
    temperature=0.4,
    max_tokens=400
)

# RAG zinciri
rag_chain = (
    RunnableMap({
        "context": hybrid_retrieve,
        "question": RunnablePassthrough()
    })
    | prompt
    | llm
)

# Flask API iÃ§in dÄ±ÅŸa aktarÄ±lan fonksiyon
def get_rag_answer(query: str):
    global memory
    # HafÄ±zaya soruyu ekle
    memory.save_context({"question": query}, {"answer": ""})
    # YanÄ±t Ã¼ret
    result = rag_chain.invoke(query).content
    # HafÄ±zaya yanÄ±tÄ± ekle
    memory.save_context({"question": query}, {"answer": result})
    return result


